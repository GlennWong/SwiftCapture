import Foundation
import ScreenCaptureKit
@preconcurrency import AVFoundation
import AppKit

/// Legacy screen recorder implementation
/// This will be refactored in later tasks into a more modular architecture
class LegacyScreenRecorder {
    
    // éŸ³é¢‘è®¾å¤‡åˆ‡æ¢åŠŸèƒ½
    static func switchAudioOutput(to deviceName: String) {
        let task = Process()
        task.executableURL = URL(fileURLWithPath: "/opt/homebrew/bin/SwitchAudioSource")
        task.arguments = ["-s", deviceName]
        do {
            try task.run()
            task.waitUntilExit()
            print("âœ… åˆ‡æ¢éŸ³é¢‘è¾“å‡ºè®¾å¤‡åˆ°ï¼š\(deviceName)")
        } catch {
            print("âš ï¸ åˆ‡æ¢éŸ³é¢‘è¾“å‡ºè®¾å¤‡å¤±è´¥: \(error)")
        }
    }
    
    @available(macOS 12.3, *)
    static func recordWithArea(durationMs: Int, outputPath: String?, fullScreen: Bool, areaString: String?, screenIndex: Int, fps: Int = 30, quality: VideoQuality = .medium, format: OutputFormat = .mov, showCursor: Bool = false) async {
        do {
            let content: SCShareableContent
            do {
                content = try await SCShareableContent.excludingDesktopWindows(false, onScreenWindowsOnly: true)
            } catch {
                print("âŒ è·å–å…±äº«å†…å®¹å¤±è´¥: \(error)")
                return
            }

            guard content.displays.count >= screenIndex else {
                print("âŒ æ‰¾ä¸åˆ°ç¬¬\(screenIndex)ä¸ªæ˜¾ç¤ºå™¨")
                return
            }
            let targetDisplay = content.displays[screenIndex - 1] // Convert to 0-based index
            let display = targetDisplay

            let config = SCStreamConfiguration()
            
            // é€šè¿‡ NSScreen è·å–å®é™…åƒç´ å°ºå¯¸ï¼ˆè€ƒè™‘ Retina ç¼©æ”¾ï¼‰
            let screens = NSScreen.screens
            guard screens.count >= screenIndex else {
                print("âŒ NSScreen æ‰¾ä¸åˆ°ç¬¬\(screenIndex)ä¸ªæ˜¾ç¤ºå™¨")
                return
            }
            let screen = screens[screenIndex - 1] // Convert to 0-based index
            let scaleFactor = screen.backingScaleFactor
            let logicalWidth = Int(screen.frame.width)
            let logicalHeight = Int(screen.frame.height)
            let actualWidth = Int(Double(logicalWidth) * scaleFactor)
            let actualHeight = Int(Double(logicalHeight) * scaleFactor)
            
            print("ğŸ–¥ï¸ æ˜¾ç¤ºå™¨ä¿¡æ¯:")
            print("   NSScreen é€»è¾‘å°ºå¯¸: \(logicalWidth) Ã— \(logicalHeight)")
            print("   ç¼©æ”¾å› å­: \(scaleFactor)")
            print("   å®é™…åƒç´ å°ºå¯¸: \(actualWidth) Ã— \(actualHeight)")
            print("   ScreenCaptureKit å°ºå¯¸: \(display.width) Ã— \(display.height)")
            
            let captureWidth: Int
            let captureHeight: Int
            let sourceRect: CGRect
            
            if let areaString = areaString {
                // è§£æåŒºåŸŸå­—ç¬¦ä¸² "x:y:width:height"
                let components = areaString.split(separator: ":").compactMap { Int($0) }
                guard components.count == 4 else {
                    print("âŒ åŒºåŸŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º x:y:width:height")
                    return
                }
                
                let x = components[0]
                let y = components[1] 
                let width = components[2]
                let height = components[3]
                
                print("ğŸ“¹ æŒ‡å®šå½•åˆ¶åŒºåŸŸ:")
                print("   åŒºåŸŸï¼ˆåƒç´ åæ ‡ï¼‰: \(x):\(y):\(width):\(height)")
                
                // éªŒè¯åŒºåŸŸæ˜¯å¦åœ¨å±å¹•èŒƒå›´å†…
                if x + width > actualWidth || y + height > actualHeight {
                    print("âŒ å½•åˆ¶åŒºåŸŸè¶…å‡ºå±å¹•èŒƒå›´")
                    print("   å±å¹•å°ºå¯¸: \(actualWidth) Ã— \(actualHeight)")
                    print("   è¯·æ±‚åŒºåŸŸ: \(x + width) Ã— \(y + height)")
                    return
                }
                
                // è¾“å‡ºå°ºå¯¸å°±æ˜¯æŒ‡å®šçš„åƒç´ å°ºå¯¸
                captureWidth = width
                captureHeight = height
                
                // sourceRect éœ€è¦è½¬æ¢ä¸ºé€»è¾‘åæ ‡ç»™ ScreenCaptureKit
                let logicalX = Double(x) / scaleFactor
                let logicalY = Double(y) / scaleFactor
                let logicalW = Double(width) / scaleFactor
                let logicalH = Double(height) / scaleFactor
                sourceRect = CGRect(x: logicalX, y: logicalY, width: logicalW, height: logicalH)
                
                print("   é€»è¾‘åæ ‡: \(logicalX):\(logicalY):\(logicalW):\(logicalH)")
                print("   è¾“å‡ºå°ºå¯¸: \(captureWidth) Ã— \(captureHeight)")
                
            } else if fullScreen {
                // æ•´å±å½•åˆ¶
                captureWidth = actualWidth
                captureHeight = actualHeight
                sourceRect = CGRect(x: 0, y: 0, width: logicalWidth, height: logicalHeight)
            } else {
                // 3:4 ç«–å±æ¯”ä¾‹è£å‰ªï¼Œä»å·¦ä¸Šè§’å¼€å§‹æˆªå–
                captureHeight = actualHeight
                captureWidth = actualHeight * 3 / 4
                let logicalCropWidth = logicalHeight * 3 / 4
                sourceRect = CGRect(x: 0, y: 0, width: logicalCropWidth, height: logicalHeight)
            }
            
            config.sourceRect = sourceRect
            config.width = captureWidth
            config.height = captureHeight
            
            print("ğŸ“¹ å½•åˆ¶é…ç½®:")
            print("   æºåŒºåŸŸï¼ˆé€»è¾‘åæ ‡ï¼‰: \(sourceRect)")
            print("   è¾“å‡ºå°ºå¯¸ï¼ˆå®é™…åƒç´ ï¼‰: \(captureWidth) Ã— \(captureHeight)")
            config.pixelFormat = kCVPixelFormatType_32BGRA
            config.minimumFrameInterval = CMTime(value: 1, timescale: CMTimeScale(fps))
            
            // å…³é”®ï¼šç¡®ä¿æ•è·é«˜åˆ†è¾¨ç‡å†…å®¹
            config.scalesToFit = false  // ä¸è¦ç¼©æ”¾ä»¥é€‚åº”
            
            // æé«˜è´¨é‡è®¾ç½®
            if #available(macOS 14.0, *) {
                config.queueDepth = 8
            }
            config.colorSpaceName = CGColorSpace.displayP3
            config.backgroundColor = CGColor.clear
            config.showsCursor = showCursor

            if #available(macOS 13.0, *) {
                config.capturesAudio = true
            }

            // è®¾ç½®è¾“å‡ºè·¯å¾„
            let outputURL: URL
            if let path = outputPath, !path.isEmpty {
                outputURL = URL(fileURLWithPath: path)
            } else {
                outputURL = URL(fileURLWithPath: FileManager.default.currentDirectoryPath)
                    .appendingPathComponent("screenRecording.mov")
            }

            if FileManager.default.fileExists(atPath: outputURL.path) {
                try FileManager.default.removeItem(at: outputURL)
            }

            let writer: AVAssetWriter
            do {
                writer = try AVAssetWriter(outputURL: outputURL, fileType: format.avFileType)
            } catch {
                print("âŒ åˆ›å»º AVAssetWriter å¤±è´¥: \(error)")
                return
            }
            
            // Create enhanced video settings using VideoSettings model
            let videoSettings = VideoSettings.default(
                fps: fps,
                quality: quality,
                resolution: CGSize(width: captureWidth, height: captureHeight),
                showCursor: showCursor
            )
            let settings = videoSettings.avSettings
            let input = AVAssetWriterInput(mediaType: .video, outputSettings: settings)
            input.expectsMediaDataInRealTime = true

            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVNumberOfChannelsKey: 2,
                AVSampleRateKey: 44100,
                AVEncoderBitRateKey: 192000
            ]
            let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioInput.expectsMediaDataInRealTime = true
            writer.add(audioInput)

            // è®¾ç½®åƒç´ ç¼“å†²åŒºå±æ€§ä»¥åŒ¹é…æºæ ¼å¼
            let pixelBufferAttributes: [String: Any] = [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA,
                kCVPixelBufferWidthKey as String: captureWidth,
                kCVPixelBufferHeightKey as String: captureHeight,
                kCVPixelBufferIOSurfacePropertiesKey as String: [:]
            ]
            let adaptor = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: input, sourcePixelBufferAttributes: pixelBufferAttributes)
            writer.add(input)
            writer.startWriting()

            class Delegate: NSObject, SCStreamOutput {
                let input: AVAssetWriterInput
                let audioInput: AVAssetWriterInput
                let adaptor: AVAssetWriterInputPixelBufferAdaptor
                var startTime: CMTime?
                let writer: AVAssetWriter

                init(input: AVAssetWriterInput, audioInput: AVAssetWriterInput, adaptor: AVAssetWriterInputPixelBufferAdaptor, writer: AVAssetWriter) {
                    self.input = input
                    self.audioInput = audioInput
                    self.adaptor = adaptor
                    self.writer = writer
                }

                func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {
                    let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
                    if startTime == nil {
                        startTime = timestamp
                        writer.startSession(atSourceTime: startTime!)
                    }

                    switch outputType {
                    case .screen:
                        guard let pixelBuffer = sampleBuffer.imageBuffer else { return }
                        if input.isReadyForMoreMediaData {
                            adaptor.append(pixelBuffer, withPresentationTime: timestamp)
                        }
                    case .audio:
                        if audioInput.isReadyForMoreMediaData {
                            audioInput.append(sampleBuffer)
                        }
                    default:
                        break
                    }
                }
            }

            let delegate = Delegate(input: input, audioInput: audioInput, adaptor: adaptor, writer: writer)

            let stream = SCStream(
                filter: SCContentFilter(display: display, excludingWindows: []),
                configuration: config,
                delegate: nil
            )

            // ä½¿ç”¨ä¸“ç”¨çš„é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—å¤„ç†è§†é¢‘æ•°æ®
            let videoQueue = DispatchQueue(label: "videoQueue", qos: .userInitiated)
            let audioQueue = DispatchQueue(label: "audioQueue", qos: .userInitiated)
            
            do {
                try stream.addStreamOutput(delegate, type: .screen, sampleHandlerQueue: videoQueue)
            } catch {
                print("âŒ æ·»åŠ è§†é¢‘æµè¾“å‡ºå¤±è´¥: \(error)")
                return
            }

            do {
                if #available(macOS 13.0, *) {
                    try stream.addStreamOutput(delegate, type: SCStreamOutputType.audio, sampleHandlerQueue: audioQueue)
                }
            } catch {
                print("âŒ æ·»åŠ éŸ³é¢‘æµè¾“å‡ºå¤±è´¥: \(error)")
                return
            }

            do {
                try await stream.startCapture()
            } catch {
                print("âŒ å¯åŠ¨å½•åˆ¶å¤±è´¥: \(error)")
                return
            }

            let durationSec = Double(durationMs) / 1000.0
            
            // Start progress indicator
            let progressIndicator = ProgressIndicator.startRecording(
                outputURL: outputURL, 
                duration: durationSec
            )
            
            // Setup graceful shutdown handling
            SignalHandler.shared.setupForRecording(
                progressIndicator: progressIndicator
            ) {
                // Graceful shutdown callback
                do {
                    try await stream.stopCapture()
                    input.markAsFinished()
                    audioInput.markAsFinished()
                } catch {
                    print("âš ï¸ Error during graceful shutdown: \(error.localizedDescription)")
                }
            }
            
            // Record for the specified duration
            try await Task.sleep(nanoseconds: UInt64(durationMs * 1_000_000))

            progressIndicator.updateProgress(message: "Stopping recording...")
            do {
                try await stream.stopCapture()
            } catch {
                print("âŒ åœæ­¢å½•åˆ¶å¤±è´¥: \(error)")
                progressIndicator.stopProgressWithError(error)
                return
            }
            input.markAsFinished()
            audioInput.markAsFinished()

            // ç”¨å¼‚æ­¥æ–¹å¼ç­‰å¾…å†™å…¥å®Œæˆ
            try await withCheckedThrowingContinuation { continuation in
                writer.finishWriting {
                    if writer.status == .completed {
                        progressIndicator.stopProgress()
                        continuation.resume()
                    } else if let error = writer.error {
                        progressIndicator.stopProgressWithError(error)
                        continuation.resume(throwing: error)
                    } else {
                        let unknownError = NSError(domain: "com.swiftcapture", code: -1, userInfo: [NSLocalizedDescriptionKey: "æœªçŸ¥å†™å…¥é”™è¯¯"])
                        progressIndicator.stopProgressWithError(unknownError)
                        continuation.resume(throwing: unknownError)
                    }
                }
            }
            
            // Clean up signal handler
            SignalHandler.shared.cleanup()

        } catch {
            print("âš ï¸ å½•åˆ¶å¤±è´¥ï¼š\(error)")
            exit(EXIT_FAILURE)
        }
    }

    @available(macOS 12.3, *)
    static func record(durationMs: Int, outputPath: String?, fullScreen: Bool, fps: Int = 30, quality: VideoQuality = .medium, format: OutputFormat = .mov, showCursor: Bool = false) async {
        do {
            let content: SCShareableContent
            do {
                content = try await SCShareableContent.excludingDesktopWindows(false, onScreenWindowsOnly: true)
            } catch {
                print("âŒ è·å–å…±äº«å†…å®¹å¤±è´¥: \(error)")
                return
            }

            guard content.displays.count >= 2 else {
                print("âŒ æ‰¾ä¸åˆ°ç¬¬äºŒä¸ªæ˜¾ç¤ºå™¨")
                return
            }
            let targetDisplay = content.displays[1]
            let display = targetDisplay

            let config = SCStreamConfiguration()
            
            // ä½¿ç”¨ä¸ .bak æ–‡ä»¶ç›¸åŒçš„æ–¹æ³•è·å–å±å¹•å°ºå¯¸
            // é€šè¿‡ NSScreen è·å–å®é™…åƒç´ å°ºå¯¸ï¼ˆè€ƒè™‘ Retina ç¼©æ”¾ï¼‰
            let screens = NSScreen.screens
            guard screens.count >= 2 else {
                print("âŒ NSScreen æ‰¾ä¸åˆ°ç¬¬äºŒä¸ªæ˜¾ç¤ºå™¨")
                return
            }
            let screen1 = screens[1]
            let scaleFactor = screen1.backingScaleFactor
            let logicalWidth = Int(screen1.frame.width)
            let logicalHeight = Int(screen1.frame.height)
            let actualWidth = Int(Double(logicalWidth) * scaleFactor)
            let actualHeight = Int(Double(logicalHeight) * scaleFactor)
            
            print("ğŸ–¥ï¸ æ˜¾ç¤ºå™¨ä¿¡æ¯:")
            print("   NSScreen é€»è¾‘å°ºå¯¸: \(logicalWidth) Ã— \(logicalHeight)")
            print("   ç¼©æ”¾å› å­: \(scaleFactor)")
            print("   å®é™…åƒç´ å°ºå¯¸: \(actualWidth) Ã— \(actualHeight)")
            print("   ScreenCaptureKit å°ºå¯¸: \(display.width) Ã— \(display.height)")
            
            let captureWidth: Int
            let captureHeight: Int
            let sourceRect: CGRect
            
            if fullScreen {
                // æ•´å±å½•åˆ¶
                captureWidth = actualWidth
                captureHeight = actualHeight
                // sourceRect ä½¿ç”¨é€»è¾‘åæ ‡
                sourceRect = CGRect(x: 0, y: 0, width: logicalWidth, height: logicalHeight)
            } else {
                // 3:4 ç«–å±æ¯”ä¾‹è£å‰ªï¼Œä»å·¦ä¸Šè§’å¼€å§‹æˆªå–
                captureHeight = actualHeight
                captureWidth = actualHeight * 3 / 4
                // sourceRect ä½¿ç”¨é€»è¾‘åæ ‡ï¼Œä½†ä¿æŒ 3:4 æ¯”ä¾‹
                let logicalCropWidth = logicalHeight * 3 / 4
                sourceRect = CGRect(x: 0, y: 0, width: logicalCropWidth, height: logicalHeight)
            }
            
            config.sourceRect = sourceRect
            config.width = captureWidth
            config.height = captureHeight
            
            print("ğŸ“¹ å½•åˆ¶é…ç½®:")
            print("   æºåŒºåŸŸï¼ˆé€»è¾‘åæ ‡ï¼‰: \(sourceRect)")
            print("   è¾“å‡ºå°ºå¯¸ï¼ˆå®é™…åƒç´ ï¼‰: \(captureWidth) Ã— \(captureHeight)")
            config.pixelFormat = kCVPixelFormatType_32BGRA
            config.minimumFrameInterval = CMTime(value: 1, timescale: CMTimeScale(fps))
            
            // å…³é”®ï¼šç¡®ä¿æ•è·é«˜åˆ†è¾¨ç‡å†…å®¹
            config.scalesToFit = false  // ä¸è¦ç¼©æ”¾ä»¥é€‚åº”
            
            // æé«˜è´¨é‡è®¾ç½®
            if #available(macOS 14.0, *) {
                config.queueDepth = 8
            }
            config.colorSpaceName = CGColorSpace.displayP3
            config.backgroundColor = CGColor.clear
            config.showsCursor = showCursor
            
            print("ğŸ“¹ å½•åˆ¶é…ç½®:")
            print("   æºåŒºåŸŸï¼ˆé€»è¾‘åæ ‡ï¼‰: \(sourceRect)")
            print("   è¾“å‡ºå°ºå¯¸ï¼ˆå®é™…åƒç´ ï¼‰: \(captureWidth) Ã— \(captureHeight)")
            print("   scalesToFit: \(config.scalesToFit)")

            if #available(macOS 13.0, *) {
                config.capturesAudio = true
            }

            // è®¾ç½®è¾“å‡ºè·¯å¾„
            let outputURL: URL
            if let path = outputPath, !path.isEmpty {
                outputURL = URL(fileURLWithPath: path)
            } else {
                outputURL = URL(fileURLWithPath: FileManager.default.currentDirectoryPath)
                    .appendingPathComponent("screenRecording.mov")
            }

            if FileManager.default.fileExists(atPath: outputURL.path) {
                try FileManager.default.removeItem(at: outputURL)
            }

            let writer: AVAssetWriter
            do {
                writer = try AVAssetWriter(outputURL: outputURL, fileType: format.avFileType)
            } catch {
                print("âŒ åˆ›å»º AVAssetWriter å¤±è´¥: \(error)")
                return
            }
            
            // Create enhanced video settings using VideoSettings model
            let videoSettings = VideoSettings.default(
                fps: fps,
                quality: quality,
                resolution: CGSize(width: captureWidth, height: captureHeight),
                showCursor: showCursor
            )
            let settings = videoSettings.avSettings
            let input = AVAssetWriterInput(mediaType: .video, outputSettings: settings)
            input.expectsMediaDataInRealTime = true

            let audioSettings: [String: Any] = [
                AVFormatIDKey: kAudioFormatMPEG4AAC,
                AVNumberOfChannelsKey: 2,
                AVSampleRateKey: 44100,
                AVEncoderBitRateKey: 192000
            ]
            let audioInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
            audioInput.expectsMediaDataInRealTime = true
            writer.add(audioInput)

            // è®¾ç½®åƒç´ ç¼“å†²åŒºå±æ€§ä»¥åŒ¹é…æºæ ¼å¼
            let pixelBufferAttributes: [String: Any] = [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA,
                kCVPixelBufferWidthKey as String: captureWidth,
                kCVPixelBufferHeightKey as String: captureHeight,
                kCVPixelBufferIOSurfacePropertiesKey as String: [:]
            ]
            let adaptor = AVAssetWriterInputPixelBufferAdaptor(assetWriterInput: input, sourcePixelBufferAttributes: pixelBufferAttributes)
            writer.add(input)
            writer.startWriting()
            // ç§»é™¤ writer.startSession(atSourceTime: .zero)

            class Delegate: NSObject, SCStreamOutput {
                let input: AVAssetWriterInput
                let audioInput: AVAssetWriterInput
                let adaptor: AVAssetWriterInputPixelBufferAdaptor
                var startTime: CMTime?
                let writer: AVAssetWriter

                init(input: AVAssetWriterInput, audioInput: AVAssetWriterInput, adaptor: AVAssetWriterInputPixelBufferAdaptor, writer: AVAssetWriter) {
                    self.input = input
                    self.audioInput = audioInput
                    self.adaptor = adaptor
                    self.writer = writer
                }

                func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {
                    let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
                    if startTime == nil {
                        startTime = timestamp
                        writer.startSession(atSourceTime: startTime!)
                    }

                    switch outputType {
                    case .screen:
                        guard let pixelBuffer = sampleBuffer.imageBuffer else { return }
                        if input.isReadyForMoreMediaData {
                            adaptor.append(pixelBuffer, withPresentationTime: timestamp)
                        }
                    case .audio:
                        if audioInput.isReadyForMoreMediaData {
                            audioInput.append(sampleBuffer)
                        }
                    default:
                        break
                    }
                }
            }

            let delegate = Delegate(input: input, audioInput: audioInput, adaptor: adaptor, writer: writer)

            let stream = SCStream(
                filter: SCContentFilter(display: display, excludingWindows: []),
                configuration: config,
                delegate: nil
            )

            // ä½¿ç”¨ä¸“ç”¨çš„é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—å¤„ç†è§†é¢‘æ•°æ®
            let videoQueue = DispatchQueue(label: "videoQueue", qos: .userInitiated)
            let audioQueue = DispatchQueue(label: "audioQueue", qos: .userInitiated)
            
            do {
                try stream.addStreamOutput(delegate, type: .screen, sampleHandlerQueue: videoQueue)
            } catch {
                print("âŒ æ·»åŠ è§†é¢‘æµè¾“å‡ºå¤±è´¥: \(error)")
                return
            }

            do {
                if #available(macOS 13.0, *) {
                    try stream.addStreamOutput(delegate, type: SCStreamOutputType.audio, sampleHandlerQueue: audioQueue)
                }
            } catch {
                print("âŒ æ·»åŠ éŸ³é¢‘æµè¾“å‡ºå¤±è´¥: \(error)")
                return
            }

            do {
                try await stream.startCapture()
            } catch {
                print("âŒ å¯åŠ¨å½•åˆ¶å¤±è´¥: \(error)")
                return
            }

            // å½•åˆ¶å‰åˆ‡æ¢éŸ³é¢‘è¾“å‡ºåˆ° BlackHole
            // switchAudioOutput(to: "BlackHole 16ch")
            
            let durationSec = Double(durationMs) / 1000.0
            
            // Start progress indicator
            let progressIndicator = ProgressIndicator.startRecording(
                outputURL: outputURL, 
                duration: durationSec
            )
            
            // Setup graceful shutdown handling
            SignalHandler.shared.setupForRecording(
                progressIndicator: progressIndicator
            ) {
                // Graceful shutdown callback
                do {
                    try await stream.stopCapture()
                    input.markAsFinished()
                    audioInput.markAsFinished()
                } catch {
                    print("âš ï¸ Error during graceful shutdown: \(error.localizedDescription)")
                }
            }
            
            // Record for the specified duration
            try await Task.sleep(nanoseconds: UInt64(durationMs * 1_000_000))

            progressIndicator.updateProgress(message: "Stopping recording...")
            do {
                try await stream.stopCapture()
            } catch {
                print("âŒ åœæ­¢å½•åˆ¶å¤±è´¥: \(error)")
                progressIndicator.stopProgressWithError(error)
                return
            }
            input.markAsFinished()
            audioInput.markAsFinished()

            // ç”¨å¼‚æ­¥æ–¹å¼ç­‰å¾…å†™å…¥å®Œæˆ
            try await withCheckedThrowingContinuation { continuation in
                writer.finishWriting {
                    if writer.status == .completed {
                        progressIndicator.stopProgress()
                        // å½•åˆ¶å®Œæˆååˆ‡æ¢å›é»˜è®¤éŸ³é¢‘è®¾å¤‡
                        // switchAudioOutput(to: "MacBook Pro Speakers")
                        continuation.resume()
                    } else if let error = writer.error {
                        progressIndicator.stopProgressWithError(error)
                        // å‡ºé”™æ—¶ä¹Ÿè¦åˆ‡æ¢å›é»˜è®¤éŸ³é¢‘è®¾å¤‡
                        // switchAudioOutput(to: "MacBook Pro Speakers")
                        continuation.resume(throwing: error)
                    } else {
                        let unknownError = NSError(domain: "com.swiftcapture", code: -1, userInfo: [NSLocalizedDescriptionKey: "æœªçŸ¥å†™å…¥é”™è¯¯"])
                        progressIndicator.stopProgressWithError(unknownError)
                        // switchAudioOutput(to: "MacBook Pro Speakers")
                        continuation.resume(throwing: unknownError)
                    }
                }
            }
            
            // Clean up signal handler
            SignalHandler.shared.cleanup()

        } catch {
            print("âš ï¸ å½•åˆ¶å¤±è´¥ï¼š\(error)")
            // å‡ºé”™æ—¶ä¹Ÿè¦åˆ‡æ¢å›é»˜è®¤éŸ³é¢‘è®¾å¤‡
            // switchAudioOutput(to: "MacBook Pro Speakers")
            exit(EXIT_FAILURE)
        }
    }
}